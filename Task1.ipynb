{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Joyce-ZhouY/ECE1512-ProjectA/blob/main/Task1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Project A: Knowledge Distillation for Building Lightweight Deep Learning Models in Visual Classification Tasks"
      ],
      "metadata": {
        "id": "6WYMfvCNPwpm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow.compat.v2 as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from typing import Union\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "tf.enable_v2_behavior()\n",
        "tf.config.run_functions_eagerly(True)\n",
        "\n",
        "builder = tfds.builder('mnist')\n",
        "BATCH_SIZE = 256\n",
        "NUM_EPOCHS = 12\n",
        "NUM_CLASSES = 10  # 10 total classes."
      ],
      "metadata": {
        "id": "vA8ppgB2P0aJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data loading"
      ],
      "metadata": {
        "id": "H2EFLQROP2R7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load train and test splits.\n",
        "def preprocess(x):\n",
        "  image = tf.image.convert_image_dtype(x['image'], tf.float32)\n",
        "  subclass_labels = tf.one_hot(x['label'], builder.info.features['label'].num_classes)\n",
        "  return image, subclass_labels\n",
        "\n",
        "mnist_train = tfds.load('mnist', split='train', shuffle_files=False).cache()\n",
        "mnist_train = mnist_train.map(preprocess)\n",
        "mnist_train = mnist_train.shuffle(builder.info.splits['train'].num_examples)\n",
        "mnist_train = mnist_train.batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "mnist_test = tfds.load('mnist', split='test').cache()\n",
        "mnist_test = mnist_test.map(preprocess).batch(BATCH_SIZE)"
      ],
      "metadata": {
        "id": "ynByMG_UP4A4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model creation"
      ],
      "metadata": {
        "id": "kAZwfvW5P63q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @test {\"output\": \"ignore\"}\n",
        "\n",
        "# Build CNN teacher.\n",
        "cnn_model = tf.keras.Sequential()\n",
        "\n",
        "# your code start from here for stpe 2\n",
        "#adding convolution layer\n",
        "cnn_model.add(tf.keras.layers.Conv2D(32,(3,3), strides=(1, 1), padding=\"same\", activation='relu', input_shape=(28,28,1)))\n",
        "#adding pooling layer\n",
        "cnn_model.add(tf.keras.layers.MaxPool2D(pool_size=(2, 2), strides=1))\n",
        "#adding convolution layer\n",
        "cnn_model.add(tf.keras.layers.Conv2D(64,(3,3), strides=(1, 1), padding=\"same\", activation='relu'))\n",
        "#adding pooling layer\n",
        "cnn_model.add(tf.keras.layers.MaxPool2D(pool_size=(2, 2), strides=2))\n",
        "#adding fully connected layer\n",
        "cnn_model.add(tf.keras.layers.Flatten())\n",
        "#randomly turn neurons on or off to improve convergence\n",
        "cnn_model.add(tf.keras.layers.Dropout(0.5))\n",
        "#fully connected to get all relavent data\n",
        "cnn_model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
        "#one more dropout\n",
        "cnn_model.add(tf.keras.layers.Dropout(0.5))\n",
        "#adding output layer\n",
        "cnn_model.add(tf.keras.layers.Dense(10))\n",
        "cnn_model.summary()\n",
        "\n",
        "# your code start from here for step 3\n",
        "\n",
        "# Build fully connected student.\n",
        "fc_model = tf.keras.Sequential()\n",
        "\n",
        "fc_model.add(tf.keras.layers.Flatten(input_shape=(28,28,1)))\n",
        "fc_model.add(tf.keras.layers.Dense(784, activation='relu'))\n",
        "fc_model.add(tf.keras.layers.Dense(784, activation='relu'))\n",
        "#adding output layer\n",
        "fc_model.add(tf.keras.layers.Dense(10))\n",
        "fc_model.summary()\n",
        "\n"
      ],
      "metadata": {
        "id": "zINgDkA7P7BP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Teacher loss function"
      ],
      "metadata": {
        "id": "8JWGucyrQGav"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def compute_teacher_loss(images, labels):\n",
        "  \"\"\"Compute subclass knowledge distillation teacher loss for given images\n",
        "     and labels.\n",
        "\n",
        "  Args:\n",
        "    images: Tensor representing a batch of images.\n",
        "    labels: Tensor representing a batch of labels.\n",
        "\n",
        "  Returns:\n",
        "    Scalar loss Tensor.\n",
        "  \"\"\"\n",
        "  # forward pass of teacher\n",
        "  # prediction of teacher model\n",
        "  subclass_logits = cnn_model(images, training=True)\n",
        "\n",
        "  # Compute cross-entropy loss for subclasses.\n",
        "  # your code start from here for step 3\n",
        "  cross_entropy_loss_value = tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=subclass_logits)\n",
        "\n",
        "  return cross_entropy_loss_value"
      ],
      "metadata": {
        "id": "DhzBP6ZLQJ57"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Student loss function"
      ],
      "metadata": {
        "id": "JS8xkuH0QbOS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @test {\"output\": \"ignore\"}\n",
        "\n",
        "# Hyperparameters for distillation (need to be tuned).\n",
        "ALPHA = 0.5 # task balance between cross-entropy and distillation loss\n",
        "DISTILLATION_TEMPERATURE = 4. #temperature hyperparameter\n",
        "\n",
        "def distillation_loss(teacher_logits: tf.Tensor, student_logits: tf.Tensor,\n",
        "                      temperature: Union[float, tf.Tensor]):\n",
        "  \"\"\"Compute distillation loss.\n",
        "\n",
        "  This function computes cross entropy between softened logits and softened\n",
        "  targets. The resulting loss is scaled by the squared temperature so that\n",
        "  the gradient magnitude remains approximately constant as the temperature is\n",
        "  changed. For reference, see Hinton et al., 2014, \"Distilling the knowledge in\n",
        "  a neural network.\"\n",
        "\n",
        "  Args:\n",
        "    teacher_logits: A Tensor of logits provided by the teacher.\n",
        "    student_logits: A Tensor of logits provided by the student, of the same\n",
        "      shape as `teacher_logits`.\n",
        "    temperature: Temperature to use for distillation.\n",
        "\n",
        "  Returns:\n",
        "    A scalar Tensor containing the distillation loss.\n",
        "  \"\"\"\n",
        "# #  # your code start from here for step 3\n",
        "  soft_targets = tf.nn.softmax(teacher_logits / temperature)\n",
        "\n",
        "  return tf.reduce_mean(\n",
        "      tf.nn.softmax_cross_entropy_with_logits(\n",
        "          soft_targets, student_logits / temperature)) * temperature ** 2\n",
        "\n",
        "\n",
        "def compute_student_loss(images, labels):\n",
        "  \"\"\"Compute subclass knowledge distillation student loss for given images\n",
        "     and labels.\n",
        "\n",
        "  Args:\n",
        "    images: Tensor representing a batch of images.\n",
        "    labels: Tensor representing a batch of labels.\n",
        "\n",
        "  Returns:\n",
        "    Scalar loss Tensor.\n",
        "  \"\"\"\n",
        "  # Compute subclass distillation loss between student subclass logits and\n",
        "  # softened teacher subclass targets probabilities.\n",
        "\n",
        "  # your code start from here for step 3\n",
        "  student_subclass_logits = fc_model(images, training=True)\n",
        "  teacher_subclass_logits = cnn_model(images, training=False)\n",
        "  distillation_loss_value = distillation_loss(teacher_subclass_logits,student_subclass_logits,DISTILLATION_TEMPERATURE)\n",
        "\n",
        "  # Compute cross-entropy loss with hard targets.\n",
        "\n",
        "  # your code start from here for step 3\n",
        "  cross_entropy_loss_value = tf.keras.losses.categorical_crossentropy(labels, student_subclass_logits,  from_logits=True)\n",
        "  loss= ALPHA*distillation_loss_value + (1-ALPHA)* cross_entropy_loss_value\n",
        "  return loss"
      ],
      "metadata": {
        "id": "lDKia4gPQMIr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train and evaluation"
      ],
      "metadata": {
        "id": "RJ1uyvurQ3w4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "\n",
        "def compute_num_correct(model, images, labels):\n",
        "  \"\"\"Compute number of correctly classified images in a batch.\n",
        "\n",
        "  Args:\n",
        "    model: Instance of tf.keras.Model.\n",
        "    images: Tensor representing a batch of images.\n",
        "    labels: Tensor representing a batch of labels.\n",
        "\n",
        "  Returns:\n",
        "    Number of correctly classified images.\n",
        "  \"\"\"\n",
        "  class_logits = model(images, training=False)\n",
        "  value= tf.reduce_sum(\n",
        "      tf.cast(tf.math.equal(tf.argmax(class_logits, -1), tf.argmax(labels, -1)),\n",
        "              tf.float32)), tf.argmax(class_logits, -1), tf.argmax(labels, -1)\n",
        "  return value\n",
        "\n",
        "\n",
        "def train_and_evaluate(model, compute_loss_fn):\n",
        "  \"\"\"Perform training and evaluation for a given model.\n",
        "\n",
        "  Args:\n",
        "    model: Instance of tf.keras.Model.\n",
        "    compute_loss_fn: A function that computes the training loss given the\n",
        "      images, and labels.\n",
        "  \"\"\"\n",
        "\n",
        "  # your code start from here for step 4\n",
        "  optimizer = tf.optimizers.Adam(learning_rate=0.001)\n",
        "  accuracy_matrix={}\n",
        "  time_matrix=[]\n",
        "  for epoch in range(1, NUM_EPOCHS + 1):\n",
        "    # Run training.\n",
        "    start_time = time.time()\n",
        "    print('Epoch {}: '.format(epoch), end='')\n",
        "    for images, labels in mnist_train:\n",
        "      with tf.GradientTape() as tape:\n",
        "         # your code start from here for step 4\n",
        "        loss_value = compute_loss_fn(images,labels)\n",
        "\n",
        "      grads = tape.gradient(loss_value, model.trainable_variables)\n",
        "      optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "    # Run evaluation.\n",
        "    num_correct = 0\n",
        "    num_total = builder.info.splits['test'].num_examples\n",
        "    for images, labels in mnist_test:\n",
        "      # your code start from here for step 4\n",
        "      num,a, b = compute_num_correct(model, images, labels)\n",
        "      num_correct += num\n",
        "    print(\"Class_accuracy: \" + '{:.2f}%'.format(\n",
        "        num_correct / num_total * 100))\n",
        "    accuracy_matrix.update({\"Student Accuracy\": num_correct / num_total * 100})\n",
        "    epoch_time = time.time() - start_time\n",
        "    time_matrix.append(epoch_time)\n",
        "    print(\"Total time per epoch: {0:.2f} seconds\".format(epoch_time))\n",
        "  total_time = np.sum(time_matrix)\n",
        "  print(\"Total time of training this model: {0:.2f} seconds\".format(total_time))\n",
        "  return accuracy_matrix\n",
        "    \n"
      ],
      "metadata": {
        "id": "EtoLbp8uQ4Vl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training models"
      ],
      "metadata": {
        "id": "NQL1lJdaRPT1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code start from here for step 5 \n",
        "import numpy as np\n",
        "# train Teacher and Student model using training dataset\n",
        "ALPHA=0.5\n",
        "DISTILLATION_TEMPERATURE = 4\n",
        "\n",
        "# train and evaluate Teacher model\n",
        "teacher_matrix = train_and_evaluate(cnn_model, compute_teacher_loss)\n",
        "#99.24%\n",
        "\n",
        "# train and evaluate Student model\n",
        "matrix = train_and_evaluate(fc_model, compute_student_loss)\n",
        "#98.84%\n",
        "\n",
        "# report test accuracy for both Teacher and Student model, with different ALPHA and TEMPERATURE"
      ],
      "metadata": {
        "id": "-AGHbyABRPz3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test accuracy vs. tempreture curve"
      ],
      "metadata": {
        "id": "sj1N38fnRTNB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code start from here for step 6\n",
        "# Plot a curve representing student test accuracy vs. temperature hyperparameters (T = 1,2,4,16,32,\n",
        "# and 64) when the task balance parameter is 0.5\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "Temperatures = [1,2,4,16,32,64]\n",
        "ALPHAS=[0, 0.2, 0.5, 0.8, 1]\n",
        "student_accuracy_matrixes = {}\n",
        "for a in ALPHAS:\n",
        "  ALPHA = a\n",
        "  accracy_among_t = []\n",
        "  for t in Temperatures:\n",
        "    DISTILLATION_TEMPERATURE = t\n",
        "    tf.keras.backend.clear_session()\n",
        "    fc_model = tf.keras.models.clone_model(fc_model)\n",
        "    matrix = train_and_evaluate(fc_model, compute_student_loss)\n",
        "    accracy_among_t.append(matrix.get('Student Accuracy').numpy())\n",
        "  student_accuracy_matrixes[ALPHA] =  accracy_among_t  \n",
        "\n",
        "\n",
        "acc_0 = student_accuracy_matrixes.get(ALPHAS[0])\n",
        "acc_1 = student_accuracy_matrixes.get(ALPHAS[1])\n",
        "acc_2 = student_accuracy_matrixes.get(ALPHAS[2])\n",
        "acc_3 = student_accuracy_matrixes.get(ALPHAS[3])\n",
        "acc_4 = student_accuracy_matrixes.get(ALPHAS[4])\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.title('Student test accracy vs temperature')\n",
        "plt.xlabel('temperature')\n",
        "plt.ylabel('Student Accuracy')\n",
        "xi = list(range(len(Temperatures)))\n",
        "\n",
        "plt.plot(acc_0,linestyle='-', marker='o', label='Student Accuracy with ALPHA = 0')\n",
        "plt.plot(acc_1,linestyle='-', marker='o', label='Student Accuracy with ALPHA = 0.2')\n",
        "plt.plot(acc_2,linestyle='-', marker='o', label='Student Accuracy with ALPHA = 0.5')\n",
        "plt.plot(acc_3,linestyle='-', marker='o', label='Student Accuracy with ALPHA = 0.8')\n",
        "plt.plot(acc_4,linestyle='-', marker='o', label='Student Accuracy with ALPHA = 1')\n",
        "\n",
        "plt.xticks(xi, Temperatures)\n",
        "plt.legend(loc='lower right')\n",
        "# plt.savefig('/content/images_minist/accuracyVStemperatures.png')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "gX4dbazrRWIz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc_0 = student_accuracy_matrixes.get(ALPHAS[0])\n",
        "acc_1 = student_accuracy_matrixes.get(ALPHAS[1])\n",
        "acc_2 = student_accuracy_matrixes.get(ALPHAS[2])\n",
        "acc_3 = student_accuracy_matrixes.get(ALPHAS[3])\n",
        "acc_4 = student_accuracy_matrixes.get(ALPHAS[4])\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.title('Student test accracy vs temperature')\n",
        "plt.xlabel('temperature')\n",
        "plt.ylabel('Student Accuracy')\n",
        "xi = list(range(len(Temperatures)))\n",
        "\n",
        "# plt.plot(acc_0,linestyle='-', marker='o', label='Student Accuracy with ALPHA = 0')\n",
        "# plt.plot(acc_1,linestyle='-', marker='o', label='Student Accuracy with ALPHA = 0.2')\n",
        "plt.plot(acc_2,linestyle='-', marker='o', label='Student Accuracy with ALPHA = 0.5')\n",
        "# plt.plot(acc_3,linestyle='-', marker='o', label='Student Accuracy with ALPHA = 0.8')\n",
        "# plt.plot(acc_4,linestyle='-', marker='o', label='Student Accuracy with ALPHA = 1')\n",
        "\n",
        "plt.xticks(xi, Temperatures)\n",
        "plt.legend(loc='lower right')\n",
        "# plt.savefig('/content/images_minist/accuracyVStemperatures(alpha=0.5).png')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "dX3HEJC2_ojh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train student from scratch"
      ],
      "metadata": {
        "id": "WNrH_1emRbGA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build fully connected student.\n",
        "fc_model_no_distillation = tf.keras.Sequential()\n",
        "fc_model_no_distillation.add(tf.keras.layers.Flatten(input_shape=(28,28,1)))\n",
        "fc_model_no_distillation.add(tf.keras.layers.Dense(784, activation='relu'))\n",
        "fc_model_no_distillation.add(tf.keras.layers.Dense(784, activation='relu'))\n",
        "fc_model_no_distillation.add(tf.keras.layers.Dense(10))\n",
        "fc_model_no_distillation.summary()\n",
        "# your code start from here for step 7\n",
        "\n",
        "\n",
        "\n",
        "#@test {\"output\": \"ignore\"}\n",
        "\n",
        "def compute_plain_cross_entropy_loss(images, labels):\n",
        "  \"\"\"Compute plain loss for given images and labels.\n",
        "\n",
        "  For fair comparison and convenience, this function also performs a\n",
        "  LogSumExp over subclasses, but does not perform subclass distillation.\n",
        "\n",
        "  Args:\n",
        "    images: Tensor representing a batch of images.\n",
        "    labels: Tensor representing a batch of labels.\n",
        "\n",
        "  Returns:\n",
        "    Scalar loss Tensor.\n",
        "  \"\"\"\n",
        "  # your code start from here for step 7\n",
        "\n",
        "  student_subclass_logits = fc_model_no_distillation(images, training=True)\n",
        "  cross_entropy_loss = tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=student_subclass_logits)\n",
        "  \n",
        "  return cross_entropy_loss\n",
        "\n",
        "\n",
        "train_and_evaluate(fc_model_no_distillation, compute_plain_cross_entropy_loss)\n",
        "\n",
        "# accracy is about 97%-98%, which is slower than the accracy of student model with KD\n"
      ],
      "metadata": {
        "id": "HjospsxIRbQ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Comparing the teacher and student model (number of of parameters and FLOPs) "
      ],
      "metadata": {
        "id": "yq3JTpQ4RuhR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras-flops\n",
        "from keras_flops import get_flops"
      ],
      "metadata": {
        "id": "Ytjg7qZqpUpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# your code start from here for step 8\n",
        "\n",
        "# count number of parameters of Teacher and student model\n",
        "cnn_model.summary() # 1,404,682\n",
        "fc_model.summary()  #1,238,730\n",
        "\n",
        "# calculate FLOPs of the teacher and student model\n",
        "teacher_flops = get_flops(cnn_model, batch_size=BATCH_SIZE)\n",
        "print(\"The number of FLOPs of the teacher model:\", teacher_flops)\n",
        "\n",
        "student_flops = get_flops(fc_model, batch_size=BATCH_SIZE)\n",
        "print(\"The number of FLOPs of the student model:\", student_flops)"
      ],
      "metadata": {
        "id": "4V8GB2yRRuxF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementing the state-of-the-art KD algorithm"
      ],
      "metadata": {
        "id": "KjwJ5oziRvRn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code start from here for step 12\n",
        "# implement KD using others novel idea\n",
        "\n",
        "# Build CNN teacher.\n",
        "cnn_model = tf.keras.Sequential()\n",
        "\n",
        "# your code start from here for stpe 2\n",
        "#adding convolution layer\n",
        "cnn_model.add(tf.keras.layers.Conv2D(32,(3,3), strides=(1, 1), padding=\"same\", activation='relu', input_shape=(28,28,1)))\n",
        "#adding pooling layer\n",
        "cnn_model.add(tf.keras.layers.MaxPool2D(pool_size=(2, 2), strides=1))\n",
        "#adding convolution layer\n",
        "cnn_model.add(tf.keras.layers.Conv2D(64,(3,3), strides=(1, 1), padding=\"same\", activation='relu'))\n",
        "#adding pooling layer\n",
        "cnn_model.add(tf.keras.layers.MaxPool2D(pool_size=(2, 2), strides=2))\n",
        "#adding fully connected layer\n",
        "cnn_model.add(tf.keras.layers.Flatten())\n",
        "#randomly turn neurons on or off to improve convergence\n",
        "cnn_model.add(tf.keras.layers.Dropout(0.5))\n",
        "#fully connected to get all relavent data\n",
        "cnn_model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
        "#one more dropout\n",
        "cnn_model.add(tf.keras.layers.Dropout(0.5))\n",
        "#adding output layer\n",
        "# cnn_model.add(tf.keras.layers.Dense(10))\n",
        "# each class is divided into 5 subclasses\n",
        "cnn_model.add(tf.keras.layers.Dense(50))\n",
        "cnn_model.summary()\n",
        "\n",
        "# your code start from here for step 3\n",
        "\n",
        "# Build fully connected student.\n",
        "fc_model = tf.keras.Sequential()\n",
        "\n",
        "fc_model.add(tf.keras.layers.Flatten(input_shape=(28,28,1)))\n",
        "fc_model.add(tf.keras.layers.Dense(784, activation='relu'))\n",
        "fc_model.add(tf.keras.layers.Dense(784, activation='relu'))\n",
        "#adding output layer\n",
        "# fc_model.add(tf.keras.layers.Dense(10))\n",
        "# each class is divided into 5 subclasses\n",
        "fc_model.add(tf.keras.layers.Dense(50))\n",
        "fc_model.summary()"
      ],
      "metadata": {
        "id": "q10lybAFRvZt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create sublabels for each image\n",
        "# Load train and test splits.\n",
        "def preprocess(x):\n",
        "  image = tf.image.convert_image_dtype(x['image'], tf.float32)\n",
        "  subclass_labels = tf.one_hot(x['label'], 5 * builder.info.features['label'].num_classes)\n",
        "  return image, subclass_labels\n",
        "\n",
        "def preprocess_test(x):\n",
        "  image = tf.image.convert_image_dtype(x['image'], tf.float32)\n",
        "  subclass_labels = tf.one_hot(x['label'], builder.info.features['label'].num_classes)\n",
        "  return image, subclass_labels\n",
        "\n",
        "mnist_train = tfds.load('mnist', split='train', shuffle_files=False).cache()\n",
        "mnist_train = mnist_train.map(preprocess)\n",
        "mnist_train = mnist_train.shuffle(builder.info.splits['train'].num_examples)\n",
        "mnist_train = mnist_train.batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "mnist_test = tfds.load('mnist', split='test').cache()\n",
        "mnist_test = mnist_test.map(preprocess_test).batch(BATCH_SIZE)"
      ],
      "metadata": {
        "id": "tReHoRjABQFJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ALPHA = 0.5\n",
        "BETA = 0.5\n",
        "TEMPERATURE = 4"
      ],
      "metadata": {
        "id": "T9_sS0FmDpY9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_teacher_loss(images, labels):\n",
        "  # forward pass of teacher\n",
        "  # prediction of teacher model\n",
        "  subclass_logits = cnn_model(images, training=True)\n",
        "  cross_entropy_loss_value = tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=subclass_logits)\n",
        "  v = subclass_logits.numpy()\n",
        "  v_norm = v/np.linalg.norm(v)\n",
        "\n",
        "  n=len(v)\n",
        "  T=1\n",
        "  loss_mat=np.zeros([n,n])\n",
        "  #e^(vi'*vj/T) matrix\n",
        "  for i in range(n):\n",
        "    vi = v_norm[i]\n",
        "    for j in range(n):\n",
        "      vj = v_norm[j]\n",
        "      loss_mat[i][j]=np.exp(np.dot(np.transpose(v[i]), v[j])/T)\n",
        "\n",
        "  loss_aux = 1/n*np.sum(np.log(np.sum(loss_mat,axis=1)))-1/T-np.log(n)\n",
        "\n",
        "  loss = cross_entropy_loss_value + BETA * loss_aux\n",
        "\n",
        "  return loss\n"
      ],
      "metadata": {
        "id": "Yr1PqQeGDLT1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def distillation_loss(teacher_logits: tf.Tensor, student_logits: tf.Tensor,\n",
        "                      temperature: Union[float, tf.Tensor]):\n",
        "  soft_targets = tf.nn.softmax(teacher_logits / temperature)\n",
        "\n",
        "  return tf.reduce_mean(\n",
        "      tf.nn.softmax_cross_entropy_with_logits(\n",
        "          soft_targets, student_logits / temperature)) * temperature ** 2\n",
        "\n",
        "\n",
        "def compute_student_loss(images, labels):\n",
        "  student_subclass_logits = fc_model(images, training=True)\n",
        "  teacher_subclass_logits = cnn_model(images, training=False)\n",
        "  distillation_loss_value = distillation_loss(teacher_subclass_logits,student_subclass_logits,DISTILLATION_TEMPERATURE)\n",
        "\n",
        "  # Compute cross-entropy loss with hard targets.\n",
        "\n",
        "  # your code start from here for step 3\n",
        "  cross_entropy_loss_value = tf.keras.losses.categorical_crossentropy(labels, student_subclass_logits,  from_logits=True)\n",
        "  loss= ALPHA*distillation_loss_value + (1-ALPHA)* cross_entropy_loss_value\n",
        "  return loss"
      ],
      "metadata": {
        "id": "c6XXXqEIWC2k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "\n",
        "def compute_num_correct(model, images, labels):\n",
        "  \"\"\"Compute number of correctly classified images in a batch.\n",
        "\n",
        "  Args:\n",
        "    model: Instance of tf.keras.Model.\n",
        "    images: Tensor representing a batch of images.\n",
        "    labels: Tensor representing a batch of labels.\n",
        "\n",
        "  Returns:\n",
        "    Number of correctly classified images.\n",
        "  \"\"\"\n",
        "  # class_logits = model(images, training=False)\n",
        "  pred_labels = model(images, training=False)\n",
        "  # pred = class_logits.numpy()\n",
        "  # new_pre = []\n",
        "  \n",
        "  # for n in range(len(pred)):\n",
        "  #   pred_10 = []\n",
        "  #   for i in range(0, 50, 5):\n",
        "  #     pred_10.append(sum(pred[n][i:i+5]))\n",
        "  #     # print(pred_10)\n",
        "  #   new_pre.append(pred_10)\n",
        "  \n",
        "  # pred_labels = tf.convert_to_tensor(new_pre, dtype=tf.float32)\n",
        "  \n",
        "  value= tf.reduce_sum(\n",
        "      tf.cast(tf.math.equal(tf.argmax(pred_labels, -1), tf.argmax(labels, -1)),\n",
        "              tf.float32)), tf.argmax(pred_labels, -1), tf.argmax(labels, -1)\n",
        "  return value\n",
        "\n",
        "\n",
        "def train_and_evaluate(model, compute_loss_fn):\n",
        "  \"\"\"Perform training and evaluation for a given model.\n",
        "\n",
        "  Args:\n",
        "    model: Instance of tf.keras.Model.\n",
        "    compute_loss_fn: A function that computes the training loss given the\n",
        "      images, and labels.\n",
        "  \"\"\"\n",
        "\n",
        "  optimizer = tf.optimizers.Adam(learning_rate=0.001)\n",
        "  accuracy_matrix={}\n",
        "  time_matrix=[]\n",
        "  for epoch in range(1, NUM_EPOCHS + 1):\n",
        "    # Run training.\n",
        "    start_time = time.time()\n",
        "    print('Epoch {}: '.format(epoch), end='')\n",
        "    for images, labels in mnist_train:\n",
        "      with tf.GradientTape() as tape:\n",
        "         # your code start from here for step 4\n",
        "        loss_value = compute_loss_fn(images,labels)\n",
        "\n",
        "      grads = tape.gradient(loss_value, model.trainable_variables)\n",
        "      optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "    # Run evaluation.\n",
        "    num_correct = 0\n",
        "    num_total = builder.info.splits['test'].num_examples\n",
        "    for images, labels in mnist_test:\n",
        "      num,a, b = compute_num_correct(model, images, labels)\n",
        "      num_correct += num\n",
        "    print(\"Class_accuracy: \" + '{:.2f}%'.format(\n",
        "        num_correct / num_total * 100))\n",
        "    accuracy_matrix.update({\"Student Accuracy\": num_correct / num_total * 100})\n",
        "    epoch_time = time.time() - start_time\n",
        "    time_matrix.append(epoch_time)\n",
        "    print(\"Total time per epoch: {0:.2f} seconds\".format(epoch_time))\n",
        "  total_time = np.sum(time_matrix)\n",
        "  print(\"Total time of training this model: {0:.2f} seconds\".format(total_time))\n",
        "  return accuracy_matrix\n",
        "    \n"
      ],
      "metadata": {
        "id": "PZpBt9ppWv17"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_EPOCHS = 15\n",
        "\n",
        "history = train_and_evaluate(cnn_model, compute_teacher_loss)\n",
        "# Class_accuracy: 99.20%\n",
        "# Total time of training this model: 2737.23 seconds"
      ],
      "metadata": {
        "id": "46RxXPRIWmek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "student_his = train_and_evaluate(fc_model, compute_student_loss)\n",
        "# Class_accuracy: 98.79%\n",
        "# Total time of training this model: 107.36 seconds"
      ],
      "metadata": {
        "id": "HC95ZpFTYEY-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# (Optional) XAI method to explain models"
      ],
      "metadata": {
        "id": "6dsOmtqdieIC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code start from here for step 13\n"
      ],
      "metadata": {
        "id": "X0IMIFW8ilPO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}