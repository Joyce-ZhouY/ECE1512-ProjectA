{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "collapsed_sections": [],
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Joyce-ZhouY/ECE1512-ProjectA/blob/main/Task2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N-dZAQIpOmux"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "tf.config.run_functions_eagerly(True)\n",
        "from tensorflow.python.ops.numpy_ops import np_config\n",
        "np_config.enable_numpy_behavior()\n",
        "from PIL import Image\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import Union\n",
        "import time\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Load and process data**"
      ],
      "metadata": {
        "id": "NJzfW5gPPsOo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! unzip /content/images.zip"
      ],
      "metadata": {
        "id": "FzbXtW64m-1V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#mkdir\n",
        "path_train = '/content/train'\n",
        "tf.io.gfile.mkdir(\n",
        "    path_train\n",
        ")\n",
        "\n",
        "path_test = '/content/test'\n",
        "tf.io.gfile.mkdir(\n",
        "    path_test\n",
        ")\n",
        "\n",
        "path_train_HP = '/content/train/HP'\n",
        "tf.io.gfile.mkdir(\n",
        "    path_train_HP\n",
        ")\n",
        "\n",
        "path_train_SSA = '/content/train/SSA'\n",
        "tf.io.gfile.mkdir(\n",
        "    path_train_SSA\n",
        ")\n",
        "\n",
        "path_test_HP = '/content/test/HP'\n",
        "tf.io.gfile.mkdir(\n",
        "    path_test_HP\n",
        ")\n",
        "\n",
        "path_test_SSA = '/content/test/SSA'\n",
        "tf.io.gfile.mkdir(\n",
        "    path_test_SSA\n",
        ")\n",
        "\n",
        "path_train_aug = '/content/train_aug'\n",
        "tf.io.gfile.mkdir(\n",
        "    path_train_aug\n",
        ")\n",
        "\n",
        "path_train_HP_aug = '/content/train_aug/HP'\n",
        "tf.io.gfile.mkdir(\n",
        "    path_train_HP_aug\n",
        ")\n",
        "\n",
        "path_train_SSA_aug = '/content/train_aug/SSA'\n",
        "tf.io.gfile.mkdir(\n",
        "    path_train_SSA_aug\n",
        ")"
      ],
      "metadata": {
        "id": "TQ6hXJmBZR24"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Augmentation**"
      ],
      "metadata": {
        "id": "fXp_bfM3NPAp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/content/annotations.csv'\n",
        "df = pd.read_csv(path)\n",
        "csv = dict(df)\n",
        "image_num = len(csv.get('Image Name'))\n",
        "\n",
        "# print(csv.get('Image Name')[0])\n",
        "\n",
        "for i in range(image_num):\n",
        "  if csv.get('Partition')[i] == \"train\":\n",
        "    if csv.get('Majority Vote Label')[i] == \"HP\":\n",
        "      src = '/content/images/' + csv.get('Image Name')[i]\n",
        "      dst = '/content/train/HP/' + csv.get('Image Name')[i]\n",
        "      tf.io.gfile.copy(src, dst, overwrite=True)\n",
        "    else:\n",
        "      src = '/content/images/' + csv.get('Image Name')[i]\n",
        "      dst = '/content/train/SSA/' + csv.get('Image Name')[i]\n",
        "      tf.io.gfile.copy(src, dst, overwrite=True)\n",
        "  else:\n",
        "    if csv.get('Majority Vote Label')[i] == \"HP\":\n",
        "      src = '/content/images/' + csv.get('Image Name')[i]\n",
        "      dst = '/content/test/HP/' + csv.get('Image Name')[i]\n",
        "      tf.io.gfile.copy(src, dst, overwrite=True)\n",
        "    else:\n",
        "      src = '/content/images/' + csv.get('Image Name')[i]\n",
        "      dst = '/content/test/SSA/' + csv.get('Image Name')[i]\n",
        "      tf.io.gfile.copy(src, dst, overwrite=True)"
      ],
      "metadata": {
        "id": "UntE85NhZ3a3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib. image as image\n",
        "from matplotlib import pyplot as plt\n",
        "from random import random\n",
        "\n",
        "path = '/content/annotations.csv'\n",
        "df = pd.read_csv(path)\n",
        "csv = dict(df)\n",
        "image_num = len(csv.get('Image Name'))\n",
        "#print(\"num=\",image_num)\n",
        "\n",
        "for i in range(image_num):\n",
        "    if csv.get('Majority Vote Label')[i] == \"HP\":\n",
        "      src = '/content/images/' + csv.get('Image Name')[i]\n",
        "      img=image.imread(src)\n",
        "      img_aug = tf.image.rot90(tf.image.random_flip_up_down(img),k=round(random()*3))+tf.random.normal(shape=[224,224,3], mean=0.0, stddev=0.1,dtype=tf.float32)\n",
        "      tf.keras.utils.save_img('/content/train_aug/HP/'+ csv.get('Image Name')[i], img_aug, data_format=None, file_format=None, scale=True)\n",
        "    else:\n",
        "      src = '/content/images/' + csv.get('Image Name')[i]\n",
        "      img=image.imread(src)\n",
        "      img_aug = tf.image.rot90(tf.image.random_flip_up_down(img),k=round(random()*3))+tf.random.normal(shape=[224,224,3], mean=0.0, stddev=0.1,dtype=tf.float32)\n",
        "      tf.keras.utils.save_img('/content/train_aug/SSA/'+ csv.get('Image Name')[i], img_aug, data_format=None, file_format=None, scale=True)\n",
        "      #break"
      ],
      "metadata": {
        "id": "JSOA2qDONcDS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IMG_SIZE = (224,224)\n",
        "# train_dir = '/content/train'\n",
        "train_dir = '/content/train_aug'\n",
        "BATCH_SIZE=32\n",
        "train_dataset = tf.keras.utils.image_dataset_from_directory(train_dir,\n",
        "                                                            shuffle=True,\n",
        "                                                            batch_size=BATCH_SIZE,\n",
        "                                                            image_size=IMG_SIZE)\n",
        "\n",
        "test_dir = '/content/test' \n",
        "test_dataset = tf.keras.utils.image_dataset_from_directory(test_dir,\n",
        "                                                            shuffle=True,\n",
        "                                                            batch_size=BATCH_SIZE,\n",
        "                                                            image_size=IMG_SIZE)"
      ],
      "metadata": {
        "id": "9W01jg8Uixa7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = train_dataset.map(lambda x, y: (tf.cast(x, tf.float32), tf.one_hot(y, 2)))\n",
        "test_dataset = test_dataset.map(lambda x, y: (tf.cast(x, tf.float32), tf.one_hot(y, 2)))"
      ],
      "metadata": {
        "id": "G9rY_bxobnXe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Accuracy matrix**"
      ],
      "metadata": {
        "id": "1TH3lTfGP3VW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import backend as K\n",
        "# The following code is from answers in this website:\n",
        "# https://datascience.stackexchange.com/questions/45165/how-to-get-accuracy-f1-precision-and-recall-for-a-keras-model\n",
        "\n",
        "def recall_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "def precision_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "def f1_m(y_true, y_pred):\n",
        "    precision = precision_m(y_true, y_pred)\n",
        "    recall = recall_m(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
        "  \n"
      ],
      "metadata": {
        "id": "igLHRqpir_E_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Teacher Model\n",
        "# -- use pre-trained ResNet50V2"
      ],
      "metadata": {
        "id": "PRKVtwUgN1X_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load pre-trained model -- Techer\n",
        "# Create base model\n",
        "teacher_resnet_50 = tf.keras.applications.resnet_v2.ResNet50V2(include_top=False, weights='imagenet')\n",
        "\n",
        "# Freeze base model\n",
        "teacher_resnet_50.trainable=False\n",
        "\n",
        "# Create new model on top.\n",
        "inputs = tf.keras.Input(shape=(224, 224, 3))\n",
        "x = teacher_resnet_50(inputs, training=False)\n",
        "x = tf.keras.layers.GlobalAveragePooling2D(dtype='float32')(x)\n",
        "x = tf.keras.layers.Dropout(0.2)(x)\n",
        "outputs = tf.keras.layers.Dense(2)(x)\n",
        "teacher_model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "# loss_fn = tf.nn.sigmoid_cross_entropy_with_logits\n",
        "# loss_fn = tf.nn.sparse_softmax_cross_entropy_with_logits \n",
        "# loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
        "\n",
        "initial_epoch = 10\n",
        "teacher_model.compile(optimizer=optimizer, \n",
        "                      loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True), \n",
        "                      metrics=['acc',f1_m,precision_m, recall_m])\n",
        "teacher_model.summary()\n"
      ],
      "metadata": {
        "id": "fakyTTbxlmll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train Teacher Model"
      ],
      "metadata": {
        "id": "c5T9BIp9N99w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = teacher_model.fit(train_dataset, epochs=initial_epoch)"
      ],
      "metadata": {
        "id": "oBzqd5ePqDlS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc = history.history['acc']\n",
        "f1 = history.history['f1_m']\n",
        "print(f1)\n",
        "precision = history.history['precision_m']\n",
        "recall = history.history['recall_m']\n",
        "loss = history.history['loss']\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(2, 1, 1)\n",
        "plt.plot(acc, label='Accuracy',linestyle='--', marker='o')\n",
        "plt.plot(f1, label='F1 Score',linestyle='--', marker='o')\n",
        "plt.plot(recall, label='Recall',linestyle='--', marker='o')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim([0, 1])\n",
        "# plt.plot([initial_epoch-1,initial_epoch-1],\n",
        "#           plt.ylim(), label='Start Fine Tuning')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training Accuracy of Teacher Model')\n",
        "\n",
        "plt.subplot(2, 1, 2)\n",
        "\n",
        "plt.ylim([10, 60])\n",
        "plt.plot(loss, label='Loss',linestyle='--', marker='o')\n",
        "# # plt.plot([initial_epoch-1,initial_epoch-1],\n",
        "# #          plt.ylim(), label='Start Fine Tuning')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training Loss of Teacher Model')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('Loss')\n",
        "# plt.savefig('/content/images_mhist/teacher_train_acc.png')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "7lVKMO3DmYz9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_his = teacher_model.evaluate(test_dataset)"
      ],
      "metadata": {
        "id": "LPH1e7jZ76VF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine tune teacher model and re-train"
      ],
      "metadata": {
        "id": "kZSiKbNtOB3I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# unfreeze the final set of CONV layers and make them trainable\n",
        "teacher_resnet_50.trainable = True\n",
        "# print(\"Number of layers in the base model: \", len(student_base_model.layers))\n",
        "\n",
        "# Fine-tune from this layer onwards\n",
        "fine_tune_at = 100\n",
        "\n",
        "for layer in teacher_resnet_50.layers[:fine_tune_at]:\n",
        "  layer.trainable = False\n",
        "\n",
        "teacher_model.compile(optimizer=tf.keras.optimizers.Adam(1e-5),  # Very low learning rate\n",
        "              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['acc',f1_m,precision_m, recall_m])\n",
        "\n",
        "teacher_model.summary()\n",
        "print(len(teacher_model.trainable_variables))\n",
        "\n",
        "# train the model again, this time fine-tuning *both* the final set\n",
        "# of CONV layers along with our set of FC layers\n",
        "\n",
        "history_finetune = teacher_model.fit(train_dataset, epochs=25)"
      ],
      "metadata": {
        "id": "oNchUMGFu2-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc += history_finetune.history['acc']\n",
        "f1 += history_finetune.history['f1_m']\n",
        "precision += history_finetune.history['precision_m']\n",
        "recall += history_finetune.history['recall_m'] \n",
        "loss += history_finetune.history['loss']\n"
      ],
      "metadata": {
        "id": "VYleBCvZrkks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(2, 1, 1)\n",
        "plt.plot(acc, label='Accuracy',linestyle='--', marker='o')\n",
        "plt.plot(f1, label='F1 Score',linestyle='--', marker='o')\n",
        "plt.plot(recall, label='Recall',linestyle='--', marker='o')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim([0, 1])\n",
        "plt.plot([initial_epoch,initial_epoch],\n",
        "          plt.ylim(), label='Start Fine Tuning')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training Accuracy of Teacher Model')\n",
        "\n",
        "plt.subplot(2, 1, 2)\n",
        "\n",
        "plt.ylim([0, 50])\n",
        "plt.plot(loss, label='Loss',linestyle='--', marker='o')\n",
        "plt.plot([initial_epoch,initial_epoch],\n",
        "         plt.ylim(), label='Start Fine Tuning')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training Loss of Teacher Model')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('Loss')\n",
        "# plt.savefig('/content/images_mhist/teacher_train_acc2.png')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "nTg9VlAFQrbT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create student model -- use pre-trained MobileNetV2**"
      ],
      "metadata": {
        "id": "_JMGkRGSOHo_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# student model\n",
        "def create_student():\n",
        "  input_size = (224,224,3)\n",
        "  student_base_model = tf.keras.applications.MobileNetV2(input_shape=input_size,\n",
        "                                                include_top=False,\n",
        "                                                weights='imagenet')\n",
        "\n",
        "  image_batch, label_batch = next(iter(train_dataset))\n",
        "  feature_batch = student_base_model(image_batch)\n",
        "  print(feature_batch.shape)\n",
        "\n",
        "  student_base_model.trainable = False\n",
        "  global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
        "  feature_batch_average = global_average_layer(feature_batch)\n",
        "  prediction_layer = tf.keras.layers.Dense(2)\n",
        "  prediction_batch = prediction_layer(feature_batch_average)\n",
        "  print(prediction_batch.shape)\n",
        "\n",
        "  inputs = tf.keras.Input(shape=input_size)\n",
        "  x = student_base_model(inputs, training=False)\n",
        "  x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "  x = tf.keras.layers.Dropout(0.2)(x)\n",
        "  outputs = prediction_layer(x)\n",
        "  student_model = tf.keras.Model(inputs, outputs)\n",
        "  # student_model.summary()\n",
        "  student_model.compile(optimizer=tf.keras.optimizers.Adam(1e-3),\n",
        "              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['acc',f1_m,precision_m, recall_m])\n",
        "  return student_model, student_base_model\n",
        "\n"
      ],
      "metadata": {
        "id": "xz1QIN5YGgJc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **compute Student Loss**"
      ],
      "metadata": {
        "id": "yxTyAEdPOwzv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate loss to train student\n",
        "ALPHA = 0.5\n",
        "TEMPERATURE = 4\n",
        "\n",
        "\n",
        "# loss = ALPHA * student_hard_target + (1-ALPHA) * KLDivergence(teacher_soft_target, student_soft_target)\n",
        "\n",
        "def distillation_loss(teacher_logits: tf.Tensor, student_logits: tf.Tensor,\n",
        "                      temperature: Union[float, tf.Tensor]):\n",
        "  soft_targets = tf.nn.softmax(teacher_logits / temperature)\n",
        "\n",
        "  return tf.reduce_mean(\n",
        "      tf.nn.softmax_cross_entropy_with_logits(\n",
        "          soft_targets, student_logits / temperature)) * temperature ** 2\n",
        "\n",
        "\n",
        "def compute_student_loss(images, labels):\n",
        "  student_subclass_logits = teacher_model(images, training=True)\n",
        "  teacher_subclass_logits = student_model(images, training=False)\n",
        "  distillation_loss_value = distillation_loss(teacher_subclass_logits,student_subclass_logits,DISTILLATION_TEMPERATURE)\n",
        "  cross_entropy_loss_value = tf.keras.losses.categorical_crossentropy(labels, student_subclass_logits,  from_logits=True)\n",
        "  loss= ALPHA*distillation_loss_value + (1-ALPHA)* cross_entropy_loss_value\n",
        "  return loss\n",
        "\n",
        "def compute_num_correct(model, images, labels):\n",
        "  class_logits = model(images, training=False)\n",
        "  value= tf.reduce_sum(\n",
        "      tf.cast(tf.math.equal(tf.argmax(class_logits, -1), tf.argmax(labels, -1)),\n",
        "              tf.float32)), tf.argmax(class_logits, -1), tf.argmax(labels, -1)\n",
        "  return value\n",
        "\n",
        "def train_and_evaluate(model, compute_loss_fn):\n",
        "  # optimizer = tf.optimizers.Adam(learning_rate=1e-3)\n",
        "  accuracy_matrix={}\n",
        "  time_matrix=[]\n",
        "  f1 = []\n",
        "  for epoch in range(1, NUM_EPOCHS + 1):\n",
        "    # Run training.\n",
        "    start_time = time.time()\n",
        "    print('Epoch {}: '.format(epoch), end='')\n",
        "    for images, labels in train_dataset:\n",
        "      with tf.GradientTape() as tape:\n",
        "         # your code start from here for step 4\n",
        "        loss_value = compute_loss_fn(images,labels)\n",
        "\n",
        "      grads = tape.gradient(loss_value, model.trainable_variables)\n",
        "      optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "    # Run evaluation.\n",
        "    num_correct = 0\n",
        "    num_total = 977\n",
        "    for images, labels in test_dataset:\n",
        "      # print(\"num_correct=\",num_correct)\n",
        "      num,a, b = compute_num_correct(model, images, labels)\n",
        "      num_correct += num\n",
        "    print(\"Class_accuracy: \" + '{:.2f}%'.format(\n",
        "        num_correct / num_total * 100))\n",
        "    accuracy_matrix.update({\"Student Accuracy\": num_correct / num_total * 100})\n",
        "    epoch_time = time.time() - start_time\n",
        "    time_matrix.append(epoch_time)\n",
        "    print(\"Total time per epoch: {0:.2f} seconds\".format(epoch_time))\n",
        "    y_pred = student_model(images)\n",
        "    y_true = labels\n",
        "    f1_score = f1_m(y_true, y_pred).numpy()\n",
        "    f1.append(f1_score)\n",
        "    print(\"F1_Score: \" + '{:.2f}'.format(f1_score))\n",
        "  total_time = np.sum(time_matrix)\n",
        "  print(\"Total time of training this model: {0:.2f} seconds\".format(total_time))\n",
        "  return accuracy_matrix, f1\n"
      ],
      "metadata": {
        "id": "ho7frG_JP3gP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Train and evaluate student model**"
      ],
      "metadata": {
        "id": "-vzfFGxkO3Ns"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_EPOCHS= 10\n",
        "optimizer = tf.optimizers.Adam(learning_rate=1e-3)\n",
        "student_model, student_base_model = create_student()\n",
        "student_model.summary()\n",
        "# train and save student model\n",
        "checkpoint_path = '/content/student_model.ckpt'\n",
        "model_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
        "                                                 save_weights_only=True,\n",
        "                                                 verbose=1)\n",
        "# matrix = train_and_evaluate(student_model,compute_student_loss)\n",
        "matrix = student_model.fit(train_dataset,\n",
        "                           epochs = NUM_EPOCHS,\n",
        "                           callbacks=[model_callback])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "IuDrP-uj6apC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine tune student model "
      ],
      "metadata": {
        "id": "YZ1jwD0AO8EQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fine_tune(model, base_model):\n",
        "  # unfreeze the final set of CONV layers and make them trainable\n",
        "  base_model.trainable = True\n",
        "  # print(\"Number of layers in the base model: \", len(student_base_model.layers))\n",
        "\n",
        "  # Fine-tune from this layer onwards\n",
        "  fine_tune_at = 100\n",
        "\n",
        "  for layer in base_model.layers[:fine_tune_at]:\n",
        "    layer.trainable = False\n",
        "\n",
        "  optimizer = tf.optimizers.Adam(learning_rate=1e-4)\n",
        "  model.compile(optimizer=optimizer, \n",
        "                loss = None,\n",
        "                # metrics=['acc'])\n",
        "                metrics = None)\n",
        "\n",
        "  # model.summary()\n",
        "  return model"
      ],
      "metadata": {
        "id": "FtJq0NOIJVRn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_EPOCHS= 25\n",
        "ALPHA = 0.5\n",
        "DISTILLATION_TEMPERATURE = 4\n",
        "optimizer = tf.optimizers.Adam(learning_rate=1e-4)\n",
        "student_model = fine_tune(student_model, student_base_model)\n",
        "matrix_fine, f1_fine = train_and_evaluate(student_model,compute_student_loss)\n"
      ],
      "metadata": {
        "id": "HcqGpIbPvW3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Test accuracy vs. Tempreture curve**"
      ],
      "metadata": {
        "id": "V9lz7nNsPZ_s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Temperatures = [1,2,4,16,32,64]\n",
        "ALPHA=0.5\n",
        "student_accuracy_matrixes = []\n",
        "student_f1_scores = []\n",
        "# latest = tf.train.latest_checkpoint(checkpoint_path)\n",
        "for t in Temperatures:\n",
        "  TEMPERATURE = t\n",
        "  tf.keras.backend.clear_session()\n",
        "  student_model,student_base_model = create_student()\n",
        "  student_model.load_weights(checkpoint_path)\n",
        "  # fine tuning student model with different T\n",
        "  NUM_EPOCHS= 25\n",
        "  optimizer = tf.optimizers.Adam(learning_rate=1e-4)\n",
        "  student_model = fine_tune(student_model, student_base_model)\n",
        "  matrix_fine,f1_fine = train_and_evaluate(student_model,compute_student_loss)\n",
        "  student_accuracy_matrixes.append(matrix_fine.get('Student Accuracy').numpy())\n",
        "  student_f1_scores.append(f1_fine)\n",
        "  "
      ],
      "metadata": {
        "id": "DKTwUndaPbej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(student_f1_scores)\n",
        "student_f1_scores = [max(f1) * 100 for f1 in student_f1_scores]\n",
        "# student_f1_scores = [f1 * 100 for f1 in student_f1_scores]\n",
        "\n",
        "print(student_f1_scores)\n",
        "print(student_accuracy_matrixes)\n",
        "\n",
        "plt.title('Student test accracy vs temperature')\n",
        "plt.xlabel('temperature')\n",
        "plt.ylabel('Student Accuracy')\n",
        "xi = list(range(len(Temperatures)))\n",
        "plt.plot(xi,student_accuracy_matrixes,linestyle='--', marker='o', color='b', label='Accuracy')\n",
        "plt.plot(xi,student_f1_scores ,linestyle='--', marker='o', color='r', label='F1 Scores')\n",
        "plt.legend(loc='upper right')\n",
        "plt.xticks(xi, Temperatures)\n",
        "# plt.savefig('/content/images_mhist/studentAcc_vs_temperatures.png')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8o0cwbfnaRQE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Comparing the teacher and student model \n",
        "## (number of of parameters and FLOPs)"
      ],
      "metadata": {
        "id": "GwC6xrenPA61"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras-flops\n",
        "from keras_flops import get_flops"
      ],
      "metadata": {
        "id": "grjIrBaJD3QQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compare teacher and student model\n",
        "\n",
        "# count number of parameters of Teacher and student model\n",
        "teacher_model.summary() # Trainable params: \n",
        "student_model.summary()  # Trainable params:\n",
        "\n",
        "# calculate FLOPs of the teacher and student model\n",
        "teacher_flops = get_flops(teacher_model, batch_size=BATCH_SIZE)\n",
        "print(\"The number of FLOPs of the teacher model:\", teacher_flops)\n",
        "# 223623992512\n",
        "\n",
        "student_flops = get_flops(student_model, batch_size=BATCH_SIZE)\n",
        "print(\"The number of FLOPs of the student model:\", student_flops)\n",
        "# 19604230144"
      ],
      "metadata": {
        "id": "aZ8tKKvoDkJG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Train student from scratch**"
      ],
      "metadata": {
        "id": "Fb0HHw8fPPjU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.python import train\n",
        "# student from scratch\n",
        "input_size = (224,224,3)\n",
        "epoch = 10\n",
        "student_base_model = tf.keras.applications.MobileNetV2(input_shape=input_size,\n",
        "                                               include_top=False,\n",
        "                                               weights='imagenet')\n",
        "image_batch, label_batch = next(iter(train_dataset))\n",
        "feature_batch = student_base_model(image_batch)\n",
        "\n",
        "\n",
        "student_base_model.trainable = False\n",
        "global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
        "feature_batch_average = global_average_layer(feature_batch)\n",
        "prediction_layer = tf.keras.layers.Dense(2)\n",
        "prediction_batch = prediction_layer(feature_batch_average)\n",
        "print(prediction_batch.shape)\n",
        "\n",
        "inputs = tf.keras.Input(shape=input_size)\n",
        "x = student_base_model(inputs, training=False)\n",
        "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "x = tf.keras.layers.Dropout(0.2)(x)\n",
        "outputs = prediction_layer(x)\n",
        "student_no_distillation = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
        "student_no_distillation.compile(optimizer=optimizer, \n",
        "                      loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True), \n",
        "                      metrics=['acc',f1_m,precision_m, recall_m])\n",
        "\n",
        "\n",
        "student_no_distillation.summary()\n",
        "student_no_distillation.fit(train_dataset, epochs=epoch)"
      ],
      "metadata": {
        "id": "xCMR9cjzDLp5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fine tuining the no distillation student model\n",
        "student_base_model.trainable = True\n",
        "# print(\"Number of layers in the base model: \", len(student_base_model.layers))\n",
        "\n",
        "# Fine-tune from this layer onwards\n",
        "fine_tune_at = 100\n",
        "\n",
        "for layer in student_base_model.layers[:fine_tune_at]:\n",
        "  layer.trainable = False\n",
        "\n",
        "optimizer=tf.keras.optimizers.Adam(1e-4)\n",
        "epoch = 25\n",
        "student_no_distillation.compile(optimizer=optimizer, \n",
        "                      loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True), \n",
        "                      metrics=['acc'])\n",
        "\n",
        "student_no_distillation.summary()\n",
        "student_no_distillation.fit(train_dataset, epochs=epoch)\n",
        "# student_base_model.summary()\n"
      ],
      "metadata": {
        "id": "_n53_3bDGKVf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}